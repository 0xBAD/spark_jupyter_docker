{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('abc').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_csv(spark, file):\n",
    "    try: \n",
    "        return spark.read.format(\"csv\")\\\n",
    "      .option(\"sep\", \";\")\\\n",
    "      .option(\"inferSchema\", \"true\")\\\n",
    "      .option(\"header\", \"true\")\\\n",
    "      .load(file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error while reading csv file {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(user_id=1, login_datetime=datetime.datetime(2020, 3, 12, 18, 26, 24, 239000))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logins = load_data_csv(spark, \"logins.csv\")\n",
    "logins.createOrReplaceTempView(\"logins\")\n",
    "logins.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(user_id=1, logout_datetime=datetime.datetime(2020, 3, 12, 18, 30, 26, 235000))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logouts = load_data_csv(spark, \"logouts.csv\") \n",
    "logouts.createOrReplaceTempView(\"logouts\")\n",
    "logouts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(user_id=1, payment_datetime=datetime.datetime(2020, 3, 12, 18, 32, 57, 315000), payment_sum=10)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payments = load_data_csv(spark, \"payments.csv\")\n",
    "payments.createOrReplaceTempView(\"payments\")\n",
    "payments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|login_date|user_id|\n",
      "+----------+-------+\n",
      "|2020-03-14|      2|\n",
      "|2020-03-14|     10|\n",
      "|2020-03-13|      2|\n",
      "|2020-03-13|     10|\n",
      "+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SQL\n",
    "Нужно посчитать DAU (day active users - уникальные пользователи которые входили в игру за день) \n",
    "по дням за последние 30 дней. \n",
    "Без дополнительных условий;\n",
    "Ограничиваем только тех пользователей у которых был login за последние 24 часа;\n",
    "Учитываем только тех пользователей которые суммарно заплатили за последние 30 дней более 100$\n",
    "\"\"\"\n",
    "df = spark.sql(\"\"\"\n",
    " select to_date(login_datetime,'yyyy-mm-dd') as login_date, \n",
    "    l.user_id\n",
    "    from (select user_id, login_datetime \n",
    "            from logins \n",
    "            where login_datetime >= date_add(current_timestamp(), -1) ) l\n",
    "    left join (select \n",
    "                user_id, sum(payment_sum) p_sum\n",
    "                from payments \n",
    "                where payment_datetime >= date_add(current_timestamp(), -30)\n",
    "                group by user_id\n",
    "                having sum(payment_sum) > 100\n",
    "    )\n",
    "    p on p.user_id=l.user_id\n",
    "    where login_datetime >= date_add(current_timestamp(), -30) and p_sum is not null\n",
    "    group by to_date(login_datetime,'yyyy-mm-dd'), l.user_id\n",
    "    order by 1 desc, 2\n",
    "\"\"\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------+\n",
      "|user_id|   login_d|count(1)|\n",
      "+-------+----------+--------+\n",
      "|     10|2020-03-14|       2|\n",
      "|     10|2020-03-13|       2|\n",
      "|     10|2020-03-12|       2|\n",
      "|     10|2020-03-11|       2|\n",
      "|     10|2020-03-10|       2|\n",
      "|     10|2020-03-09|       2|\n",
      "|     10|2020-03-08|       2|\n",
      "|     10|2020-03-07|       2|\n",
      "|     10|2020-03-06|       2|\n",
      "|     10|2020-03-05|       2|\n",
      "|     10|2020-03-04|       2|\n",
      "|     10|2020-03-03|       2|\n",
      "|     10|2020-03-02|       2|\n",
      "|     10|2020-03-01|       2|\n",
      "|     10|2020-02-29|       2|\n",
      "|     10|2020-02-28|       2|\n",
      "|     10|2020-02-27|       2|\n",
      "|     10|2020-02-26|       2|\n",
      "|     10|2020-02-25|       2|\n",
      "|     10|2020-02-24|       2|\n",
      "+-------+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# logins check\n",
    "spark.sql(\"\"\"\n",
    "select user_id, to_date(login_datetime,'yyyy-mm-dd') login_d, count(1)\n",
    "from logins\n",
    "where user_id in (10)\n",
    "group by user_id, to_date(login_datetime,'yyyy-mm-dd')\n",
    "order by 1, 2 desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|user_id|  character|\n",
      "+-------+-----------+\n",
      "|      1|    warrior|\n",
      "|      2|     wizard|\n",
      "|      3|  barbarian|\n",
      "|      4|      rogue|\n",
      "|      5|  barbarian|\n",
      "|      6|      rogue|\n",
      "|      7|  barbarian|\n",
      "|      8|necromancer|\n",
      "|      9|necromancer|\n",
      "|     10|  barbarian|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Нужно узнать, какой “основной” класс персонажа у каждого юзера. \n",
    "То есть, за какой класс он играл больше всего времени за весь период. \n",
    "На выходе должны получить таблицу user_id; character\n",
    "'''\n",
    "from pyspark.sql.functions import max, sum, rank\n",
    "from pyspark.sql import Window\n",
    "df = load_data_csv(spark,\"user_classes.csv\")\n",
    "df_agg = df.groupBy(\"user_id\",\"character\").\\\n",
    "            agg(\n",
    "            sum(\"session_time\").alias(\"ch_ses\"))\n",
    "            \n",
    "windowSpec  = Window.orderBy(\"ch_ses\").partitionBy(\"user_id\")\n",
    "df_favorite_character = df_agg.withColumn(\"character_rank\",\n",
    "                                          rank().over(windowSpec))\\\n",
    "                        .filter(\"character_rank = 1\")\\\n",
    "                        .drop(\"ch_ses\",\"character_rank\").orderBy(\"user_id\")\n",
    "\n",
    "df_favorite_character.show()\n",
    "\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|diff| arpu|\n",
      "+----+-----+\n",
      "|  12| 2.58|\n",
      "|   9| 3.22|\n",
      "|  39|24.97|\n",
      "|  14| 1.79|\n",
      "|  36| 8.83|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Имеем таблицу с платежами: user_id, pay_dt, pay_sum и таблицу с регистрациями: \n",
    "user_id; reg_dt. Нужно посчитать ARPU (Average Revenue Per User) \n",
    "для каждого дня жизни этих пользователей. \n",
    "День жизни пользователей -- количество дней между датой платежа и датой регистрации пользователя. \n",
    "На выходе нужно получить следующую таблицу:\n",
    "diff; arpu, где diff - количество дней между датой платежа и датой регистрации, \n",
    "оно же день жизни пользователей.\n",
    "Таким образом эта таблица будет показывать, \n",
    "сколько в среднем каждый пользователь приносит денег на 0 день жизни, на 1 день жизни и так далее.\n",
    "Важный момент: стоит обратить внимание, что далеко не все пользователи должны попадать в выборку для расчета. \n",
    "Например, считая ARPU за 90й день жизни будет некорректно брать тех юзеров, \n",
    "которые зарегистрировали вчера, т.к. они просто еще попросту не прожили 90 дней и никак не могли заплатить.\n",
    "\"\"\"\n",
    "from pyspark.sql.functions import max, sum, rank, count, lit, datediff, round\n",
    "from pyspark.sql import Window\n",
    "\n",
    "df_user_reg = load_data_csv(spark,\"user_reg.csv\")\n",
    "df_payments = load_data_csv(spark,\"payments.csv\")\n",
    "\n",
    "df_payments_agg = df_payments.withColumn(\"pay_dt\",expr(\"to_date(payment_datetime)\"))\\\n",
    "    .groupBy(\"pay_dt\", \"user_id\").agg(sum(\"payment_sum\").alias(\"rev\"))\n",
    "\n",
    "\n",
    "df_user_reg_agg = df_user_reg.withColumn(\"reg_dt\",expr(\"to_date(reg_dt)\"))\\\n",
    "                    .join(df_payments_agg,df_payments_agg.user_id == df_user_reg.user_id )\\\n",
    "                    .withColumn(\"diff\",datediff(df_payments_agg.pay_dt, col(\"reg_dt\")))\\\n",
    "                    .groupBy(\"diff\").agg((round(sum(\"rev\")/col(\"diff\"), 2)).alias(\"arpu\"))\n",
    "    \n",
    "df_user_reg_agg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-------+--------------------+-----------+\n",
      "|user_id|              reg_dt|user_id|    payment_datetime|payment_sum|\n",
      "+-------+--------------------+-------+--------------------+-----------+\n",
      "|      1|2020-02-03 16:07:...|      1|2020-03-13 14:24:...|          1|\n",
      "|      2|2020-02-02 16:07:...|      2|2020-03-12 14:24:...|         40|\n",
      "|      3|2020-02-01 16:07:...|      3|2020-03-11 14:24:...|          3|\n",
      "|      4|2020-01-31 16:07:...|      4|2020-03-10 14:24:...|          4|\n",
      "|      5|2020-01-30 16:07:...|      5|2020-03-09 14:24:...|          0|\n",
      "|      6|2020-01-29 16:07:...|      6|2020-03-08 14:24:...|          4|\n",
      "|      7|2020-01-28 16:07:...|      7|2020-03-07 14:24:...|          5|\n",
      "|      8|2020-01-27 16:07:...|      8|2020-03-06 14:24:...|          4|\n",
      "|      9|2020-01-26 16:07:...|      9|2020-03-05 14:24:...|          3|\n",
      "|     10|2020-01-25 16:07:...|     10|2020-03-04 14:24:...|         12|\n",
      "|      1|2020-02-03 16:07:...|      1|2020-03-13 14:24:...|          2|\n",
      "|      2|2020-02-02 16:07:...|      2|2020-03-12 14:24:...|          5|\n",
      "|      3|2020-02-01 16:07:...|      3|2020-03-11 14:24:...|          1|\n",
      "|      4|2020-01-31 16:07:...|      4|2020-03-10 14:24:...|          3|\n",
      "|      5|2020-01-30 16:07:...|      5|2020-03-09 14:24:...|          4|\n",
      "|      6|2020-01-29 16:07:...|      6|2020-03-08 14:24:...|          1|\n",
      "|      7|2020-01-28 16:07:...|      7|2020-03-07 14:24:...|          3|\n",
      "|      8|2020-01-27 16:07:...|      8|2020-03-06 14:24:...|          3|\n",
      "|      9|2020-01-26 16:07:...|      9|2020-03-05 14:24:...|          3|\n",
      "|     10|2020-01-25 16:07:...|     10|2020-03-04 14:24:...|         10|\n",
      "+-------+--------------------+-------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_check = df_user_reg.join(df_payments,df_payments.user_id == df_user_reg.user_id)\n",
    "df_check.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
